test_labels = NULL
for(author in author_dirs) {
author_name = substring(author, first=22)
files_to_add = Sys.glob(paste0(author, '/*.txt'))
file_list = append(file_list, files_to_add)
test_labels = append(test_labels, rep(author_name, length(files_to_add)))
}
# Named conversion & cleanup
all_docs = lapply(file_list, readerPlain)
names(all_docs) = file_list
names(all_docs) = sub('.txt', '', names(all_docs))
#Initialize Testing Corpus
test_corpus = Corpus(VectorSource(all_docs))
names(test_corpus) = file_list
#Tokenization of Testing Corpus
test_corpus = tm_map(test_corpus, content_transformer(tolower))
test_corpus = tm_map(test_corpus, content_transformer(removeNumbers))
test_corpus = tm_map(test_corpus, content_transformer(removePunctuation))
test_corpus = tm_map(test_corpus, content_transformer(stripWhitespace))
test_corpus = tm_map(test_corpus, content_transformer(removeWords), stopwords("SMART"))
#### Dictionary Creation ####
# We need a dictionary of terms from the training corpus
# in order to extract terms from the test corpus
reuters_dict = NULL
reuters_dict = dimnames(DTM_train)[[2]]
#Create testing DTM & matrix using dictionary words only
DTM_test = DocumentTermMatrix(test_corpus, list(dictionary=reuters_dict))
DTM_test = removeSparseTerms(DTM_test, 0.95)
#DTM_test = as.matrix(DTM_test)
DTM_train_df = as.data.frame(inspect(DTM_train))
#DTM_train$auth_name = train_labels
DTM_test_df = as.data.frame(inspect(DTM_test))
#DTM_test$auth_name = test_labels
model_NB = naiveBayes(x=DTM_train_df, y=as.factor(train_labels), laplace=2)
pred_NB = predict(model_NB, DTM_test_df)
table_NB = as.data.frame(table(pred_NB,test_labels))
plot = ggplot(table_NB)
plot + geom_tile(aes(x=test_labels, y=pred_NB, fill=Freq)) +
scale_x_discrete(name="Actual Class") +
scale_y_discrete(name="Predicted Class") +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
conf_NB = confusionMatrix(table(pred_NB,test_labels))
conf_NB_df = as.data.frame(conf_NB$byClass)
conf_NB
conf_NB_df = as.data.frame(conf_NB$byClass)
conf_NB_df[order(-conf_NB_df$Sensitivity),1:2]
conf_NB_df = as.data.frame(conf_NB$byClass)
conf_NB_df[order(-conf_NB_df$Sensitivity),1:2]
conf_NB_df[order(-conf_NB_df$Sensitivity),c(1,2,8)]
set.seed(1)
DTM_test = as.matrix(DTM_test)
DTM_train = as.matrix(DTM_train)
DTM_train
xx <- data.frame(DTM_test[,intersect(colnames(DTM_test), colnames(DTM_train))])
yy <- read.table(textConnection(""), col.names = colnames(DTM_train), colClasses = "integer")
library(plyr)
DTM_test_clean = rbind.fill(xx, yy)
DTM_test_df = as.data.frame(DTM_test_clean)
model_RF = randomForest(x=DTM_train_df, y=as.factor(train_labels), mtry=4, ntree=350)
pred_RF = predict(model_RF, data=DTM_test_clean)
table_RF = as.data.frame(table(pred_RF,test_labels))
plot = ggplot(table_RF)
conf_RF = confusionMatrix(table(pred_RF,test_labels))
conf_RF
conf_RF$overall
conf_RF_df = as.data.frame(conf_RF$byClass)
conf_RF_df[order(-conf_RF_df$Sensitivity),c(1,2,8)]
model_RF = randomForest(x=DTM_train_df, y=as.factor(train_labels), mtry=3, ntree=350)
pred_RF = predict(model_RF, data=DTM_test_clean)
table_RF = as.data.frame(table(pred_RF,test_labels))
conf_RF = confusionMatrix(table(pred_RF,test_labels))
conf_RF$overall
as.data.frame(conf_RF$byClass)
conf_RF_df
as.data.frame(conf_RF$byClass)
conf_RF = confusionMatrix(table(pred_RF,test_labels))
conf_RF_df[order(-conf_RF_df$Sensitivity),c(1,2,8)]
as.data.frame(conf_RF$byClass)
conf_RF_df[order(-conf_RF_df$Sensitivity),c(1,2,8)]
as.data.frame(conf_RF$byClass)
reuters_dict
dimnames(DTM_train)[[2]]
DTM_test = DocumentTermMatrix(test_corpus, list(dictionary=reuters_dict))
DTM_test = removeSparseTerms(DTM_test, 0.95)
DTM_train_df = as.data.frame(inspect(DTM_train))
reuters_dict = NULL
reuters_dict = dimnames(DTM_train)[[2]]
DTM_test = DocumentTermMatrix(test_corpus, list(dictionary=reuters_dict))
DTM_test = removeSparseTerms(DTM_test, 0.95)
DTM_train_df = as.data.frame(inspect(DTM_train))
library(tm)
library(e1071)
library(rpart)
library(ggplot2)
library(caret)
readerPlain = function(fname){
readPlain(elem=list(content=readLines(fname)), id=fname, language='en') }
author_dirs = Sys.glob('./ReutersC50/C50train/*')
author_dirs
file_list = NULL
train_labels = NULL
for(author in author_dirs) {
author_name = substring(author, first=23)
files_to_add = Sys.glob(paste0(author, '/*.txt'))
file_list = append(file_list, files_to_add)
train_labels = append(train_labels, rep(author_name, length(files_to_add)))
}
all_docs = lapply(file_list, readerPlain)
names(all_docs) = file_list
names(all_docs) = sub('.txt', '', names(all_docs))
#Initialize Training Corpus
train_corpus = Corpus(VectorSource(all_docs))
names(train_corpus) = file_list
#Tokenization of training Corpus
train_corpus = tm_map(train_corpus, content_transformer(tolower))
train_corpus = tm_map(train_corpus, content_transformer(removeNumbers))
train_corpus = tm_map(train_corpus, content_transformer(removePunctuation))
train_corpus = tm_map(train_corpus, content_transformer(stripWhitespace))
train_corpus = tm_map(train_corpus, content_transformer(removeWords), stopwords("SMART"))
DTM_train = DocumentTermMatrix(train_corpus)
DTM_train = removeSparseTerms(DTM_train, 0.95)
author_dirs = Sys.glob('./ReutersC50/C50test/*')
file_list = NULL
test_labels = NULL
for(author in author_dirs) {
author_name = substring(author, first=22)
files_to_add = Sys.glob(paste0(author, '/*.txt'))
file_list = append(file_list, files_to_add)
test_labels = append(test_labels, rep(author_name, length(files_to_add)))
}
# Named conversion & cleanup
all_docs = lapply(file_list, readerPlain)
names(all_docs) = file_list
names(all_docs) = sub('.txt', '', names(all_docs))
#Initialize Testing Corpus
test_corpus = Corpus(VectorSource(all_docs))
names(test_corpus) = file_list
#Tokenization of Testing Corpus
test_corpus = tm_map(test_corpus, content_transformer(tolower))
test_corpus = tm_map(test_corpus, content_transformer(removeNumbers))
test_corpus = tm_map(test_corpus, content_transformer(removePunctuation))
test_corpus = tm_map(test_corpus, content_transformer(stripWhitespace))
test_corpus = tm_map(test_corpus, content_transformer(removeWords), stopwords("SMART"))
dimnames(DTM_train)[[2]]
reuters_dict = NULL
reuters_dict = dimnames(DTM_train)[[2]]
DTM_test = DocumentTermMatrix(test_corpus, list(dictionary=reuters_dict))
DTM_test
DTM_test = removeSparseTerms(DTM_test, 0.95)
DTM_train_df = as.data.frame(inspect(DTM_train))
DTM_test_df = as.data.frame(inspect(DTM_test))
model_NB = naiveBayes(x=DTM_train_df, y=as.factor(train_labels), laplace=2)
pred_NB = predict(model_NB, DTM_test_df)
table_NB = as.data.frame(table(pred_NB,test_labels))
conf_NB = confusionMatrix(table(pred_NB,test_labels))
conf_NB_df = as.data.frame(conf_NB$byClass)
conf_NB_df[order(-conf_NB_df$Sensitivity),c(1,2,8)]
conf_NB
conf_NB_df[order(-conf_NB_df$Sensitivity),c(1,2,8)]
set.seed(1)
DTM_test = as.matrix(DTM_test)
DTM_train = as.matrix(DTM_train)
xx <- data.frame(DTM_test[,intersect(colnames(DTM_test), colnames(DTM_train))])
yy <- read.table(textConnection(""), col.names = colnames(DTM_train), colClasses = "integer")
library(plyr)
DTM_test_clean = rbind.fill(xx, yy)
DTM_test_df = as.data.frame(DTM_test_clean)
library(randomForest)
model_RF = randomForest(x=DTM_train_df, y=as.factor(train_labels), mtry=4, ntree=350)
pred_RF = predict(model_RF, data=DTM_test_clean)
table_RF = as.data.frame(table(pred_RF,test_labels))
conf_RF = confusionMatrix(table(pred_RF,test_labels))
conf_RF$overall
conf_RF_df = as.data.frame(conf_RF$byClass)
conf_RF_df[order(-conf_RF_df$Sensitivity),c(1,2,8)]
library(arules)  # has a big ecosystem of packages built around it
playlists <- read.csv("../data/playlists.csv")
playlists$user <- factor(playlists$user)
playlists <- split(x=playlists$artist, f=playlists$user)
playlists <- lapply(playlists, unique)
playtrans <- as(playlists, "transactions")
parameter=list(support=.01, confidence=.5, maxlen=4))
musicrules <- apriori(playtrans,
parameter=list(support=.01, confidence=.5, maxlen=4))
inspect(musicrules)
DTM_train_df
Carts
inspect(Carts)
library(arules)
detach(package:tm,unload=TRUE)
inspect(Carts)
Carts <- apriori(Transactions, parameter=list(support=.001, confidence=.4, maxlen=5))
inspect(Carts)
inspect(subset(Carts, subset=lift>3.5))
inspect(subset(Carts, subset=lift>4.5))
carts.sorted <- sort(Carts, by="lift")
inspect(carts.sorted)
inspect(subset(carts.sorted, subset=lift>4.5))
inspect(subset(carts.sorted, subset=lift>5))
inspect(subset(carts.sorted, subset=lift>6))
inspect(subset(carts.sorted, subset= confidence > .5 & lift>6))
inspect(subset(carts.sorted, subset= confidence > .5 & lift>10))
inspect(subset(carts.sorted, subset=lift>6))
inspect(subset(carts.sorted, subset= confidence > .5 & lift>10))
AirportInfo = read.csv("ABIA.csv", header=TRUE)
attach(AirportInfo)
names(AirportInfo)
library(mosaic)
library(foreach)
AirportInfo$Austin <- ifelse(AirportInfo$Origin == "AUS", "Yes", NA)
names(AirportInfo)
names(AirportInfo)
Delay<-AirportInfo[,c(2,23,30)]
names(Delay)
NoNA <- na.omit(Delay)
NoNA
ave.Depdelay <- aggregate(NoNA$CancellationCode, by=list(NoNA$Month), mean)
plot(ave.Depdelay,xlab="Month",ylab="Delay Time",main= "Flights from Austin: Delay time by month")
NoNA$CancellationCode
ave.Depdelay <- aggregate(NoNA$CancellationCode, by=list(NoNA$Month), count)
ave.Depdelay <- aggregate(NoNA$CancellationCode, by=list(NoNA$Month), count)
Delay<-AirportInfo[,c(2,16,30)]
names(Delay)
Delay<-AirportInfo[,c(2,23,30)]
names(Delay)
NoNA <- na.omit(Delay)
NoNA
Delay<-AirportInfo[,c(2,23,30)]
names(Delay)
ave.Depdelay <- aggregate(NoNA$Month, by=list(NoNA$Month,NoNA$CancellationCode ), count)
ave.Depdelay <- aggregate(NoNA$Month, by=list(NoNA$Month,NoNA$CancellationCode ), count)
names(AirportInfo)
Delay<-AirportInfo[,c(2,23,10,30)]
names(Delay)
NoNA <- na.omit(Delay)
NoNA
NoNA
names(Delay)
ave.Depdelay <- aggregate(NoNA$FlightNum, by=list(NoNA$Month,NoNA$CancellationCode ), count)
ave.Depdelay <- aggregate(NoNA$FlightNum, by=list(NoNA$Month,NoNA$CancellationCode), count)
names(Delay)
NoNA <- na.omit(Delay)
NoNA
names(NoNA)
names(AirportInfo)
Delay<-AirportInfo[,c(2,22,10,30)]
names(Delay)
Delay<-AirportInfo[,c(2,22,23,30)]
names(Delay)
NoNA <- na.omit(Delay)
names(NoNA)
ave.Depdelay <- aggregate(NoNA$Cancelled by=list(NoNA$Month,NoNA$CancellationCode),sum)
ave.Depdelay <- aggregate(NoNA$Cancelled, by=list(NoNA$Month,NoNA$CancellationCode),sum)
plot(ave.Depdelay,xlab="Month",ylab="Delay Time",main= "Flights from Austin: Delay time by month")
ave.Depdelay <- aggregate(NoNA$Cancelled, by=list(NoNA$Month),sum)
plot(ave.Depdelay,xlab="Month",ylab="Delay Time",main= "Flights from Austin: Delay time by month")
ave.Depdelay <- aggregate(NoNA$Cancelled, by=list(,NoNA$CancellationCode,NoNA$Month),sum)
ave.Depdelay <- aggregate(NoNA$Cancelled, by=list(,NoNA$CancellationCode,NoNA$Month),sum)
ave.Depdelay <- aggregate(NoNA$Cancelled, by=list(NoNA$CancellationCode,NoNA$Month),sum)
plot(ave.Depdelay,xlab="Month",ylab="Delay Time",main= "Flights from Austin: Delay time by month")
ave.Depdelay <- aggregate(NoNA$Cancelled, by=list(NoNA$CancellationCode),sum)
plot(ave.Depdelay,xlab="Month",ylab="Delay Time",main= "Flights from Austin: Delay time by month")
ave.Depdelay <- aggregate(NoNA$Cancelled, by=list(NoNA$Month),sum)
plot(ave.Depdelay,xlab="Month",ylab="Delay Time",main= "Flights from Austin: Delay time by month")
Cancel<-AirportInfo[,c(2,22,30)]
names(Cancel)
NoNA <- na.omit(Cancel)
names(NoNA)
ave.Depdelay <- aggregate(NoNA$Cancelled, by=list(NoNA$Month),sum)
plot(ave.Depdelay,xlab="Month",ylab="Cancellations",main= "Flights from Austin: Cancellations by month")
names(AirportInfo)
library(Hmisc)
names(AirportInfo)
DepDelayed<-AirportInfo[,c(18,22,30)]
names(DepDelayed)
NoNAOrt <- na.omit(DepDelayed)
ave.Orgdelay <- aggregate(NoNAOrt$Cancelled, by=list(NoNAOrt$Dest), sum)
plot(ave.Orgdelay,xlab="Destination",ylab="Cancellations",main= "Flights from Austin:Cancellations by Destination", las=2)
names(AirportInfo)
Flights <- AirportInfo[,c(18,,30)]
names(Cancellations)
Cancellations<-AirportInfo[,c(18,22,30)]
names(Cancellations)
FlightsToDest <- na.omit(Flights)
Flights <- AirportInfo[,c(18,30)]
FlightsToDest <- na.omit(Flights)
FlightToDest$Austin <- 1
FlightsToDest$Austin <- 1
DestFlights<- aggregate(FlightsToDest$Austin, by=list(FlightsToDest$Dest), sum)
plot(DestFlights,xlab="Destination",ylab="Total Flights",main= "Flights from Austin: Total flights to each Destination", las=2)
par(mfrow=c(1,2))
Cancellations<-AirportInfo[,c(18,22,30)]
names(Cancellations)
DestCancelled <- na.omit(Cancellations)
DestCancel <- aggregate(DestCancelled$Cancelled, by=list(DestCancelled$Dest), sum)
plot(DestCancel,xlab="Destination",ylab="Cancellations",main= "Flights from Austin:Cancellations by Destination", las=2)
Flights <- AirportInfo[,c(18,30)]
FlightsToDest <- na.omit(Flights)
FlightsToDest$Austin <- 1
DestFlights<- aggregate(FlightsToDest$Austin, by=list(FlightsToDest$Dest), sum)
plot(DestFlights,xlab="Destination",ylab="Total Flights",main= "Flights from Austin: Total flights to each Destination", las=2)
Austin[,c(9,22,10,30)]
Austin$Austin <- ifelse(Austin$Origin == "AUS", "Yes", NA)
AirportInfo[,c(4,22,30)]
names(Cancelled)
Cancelled<-AirportInfo[,c(4,22,30)]
CancelledFlights <- na.omit(Cancelled)
names(CancelledFlights)
CancelledByAirline<-AirportInfo[,c(9,22,10,30)]
CancelledByAirline
names(AirportInfo)
CancelCarrier$Total
DestCancel$Total = DestFlights$x
par(mfrow=c(1,1))
DestCancel$Total = DestFlights$x
DestCancel$Percent <- (DestCancel$x/DestCancel$Total)*100
plot(DestFlights$Group.1, DestFlights$Percent,xlab=" Destination ",ylab="Cancelleation Percentage",main= "Cancellation percentage of flights from Austin by Destination",  las=3)
DestFlights$Group.1
DestCancel$Total
DestFlights$x
DestFlights<- aggregate(FlightsToDest$Austin, by=list(FlightsToDest$Dest), sum)
plot(DestFlights,xlab="Destination",ylab="Total Flights",main= "Flights from Austin: Total flights to each Destination", las=2)
DestFlights$x
DestCancel$Total
DestCancel$x
DestCancel$x <- DestFlights$x
DestCancel$Total <- DestFlights$x
DestCancel$Percent <- (DestCancel$x/DestCancel$Total)*100
DestCancel$Percent
DestCancel$x
DestCancel$Total
DestCancel <- aggregate(DestCancelled$Cancelled, by=list(DestCancelled$Dest), sum)
DestCancel$x
DestCancel$Total <- DestFlights$x
DestCancel$x
DestCancel$Total
DestCancel$Percent <- (DestCancel$x/DestCancel$Total)*100
plot(DestFlights$Group.1, DestFlights$Percent,xlab=" Destination ",ylab="Cancelleation Percentage",main= "Cancellation percentage of flights from Austin by Destination",  las=3)
DestCancel$x
DestCancel$Total
DestFlights$Group.1
DestFlights
plot(DestFlights$Group.1, DestFlights$Percent)
plot(DestFlights$Group.1, DestFlights$Percent,xlab=" Destination ",ylab="Cancelleation Percentage",main= "Cancellation percentage of flights from Austin by Destination",  las=3)
plot(DestCancel$Group.1, DestCancel$Percent,xlab=" Destination ",ylab="Cancelleation Percentage",main= "Cancellation percentage of flights from Austin by Destination",  las=3)
AirportInfo = read.csv("ABIA.csv", header=TRUE)
attach(AirportInfo)
names(AirportInfo)
library(mosaic)
library(foreach)
AirportInfo$Austin <- ifelse(AirportInfo$Origin == "AUS", "Yes", NA)
names(AirportInfo)
Cancel<-AirportInfo[,c(2,22,30)]
names(Cancel)
NoNA <- na.omit(Cancel)
names(NoNA)
ave.Depdelay <- aggregate(NoNA$Cancelled, by=list(NoNA$Month),sum)
plot(ave.Depdelay,xlab="Month",ylab="Cancellations",main= "Flights from Austin: Cancellations by month")
library(Hmisc)
names(AirportInfo)
par(mfrow=c(1,2))
Cancellations<-AirportInfo[,c(18,22,30)]
names(Cancellations)
DestCancelled <- na.omit(Cancellations)
DestCancel <- aggregate(DestCancelled$Cancelled, by=list(DestCancelled$Dest), sum)
plot(DestCancel,xlab="Destination",ylab="Cancellations",main= "Flights from Austin:Cancellations by Destination", las=2)
Flights <- AirportInfo[,c(18,30)]
FlightsToDest <- na.omit(Flights)
FlightsToDest$Austin <- 1
DestFlights<- aggregate(FlightsToDest$Austin, by=list(FlightsToDest$Dest), sum)
DestFlights$x
plot(DestFlights,xlab="Destination",ylab="Total Flights",main= "Flights from Austin: Total flights to each Destination", las=2)
par(mfrow=c(1,1))
DestCancel$Total <- DestFlights$x
DestCancel$Percent <- (DestCancel$x/DestCancel$Total)*100
plot(DestCancel$Group.1, DestCancel$Percent,xlab=" Destination ",ylab="Cancellation Percentage",main= "Cancellation percentage of flights from Austin by Destination",  las=3)
Cancelled<-AirportInfo[,c(4,22,30)]
CancelledFlights <- na.omit(Cancelled)
names(CancelledFlights)
CountCancells <- aggregate(Cancelled$Cancelled, by=list(Cancelled$DayOfWeek), sum)
plot(CountCancells ,xlab="Day of Week ",ylab="Cancellations",main= "Cancellations by Day of Week for flights from Austin", las=2)
names(AirportInfo)
CancelledByAirline<-AirportInfo[,c(9,22,10,30)]
CancelledFlight <- na.omit(CancelledByAirline)
CancelCarrier <- aggregate(CancelledFlight$Cancelled, by=list(CancelledFlight$UniqueCarrier), sum)
FlightsByAirline<-Austin[,c(9,30)]
Flight <- na.omit(FlightsByAirline)
Flight$Austin <- 1
TotalFlights<- aggregate(Flight$Austin, by=list(Flight$UniqueCarrier), sum)
par(mfrow=c(1,2))
plot(CancelCarrier ,xlab="Airline ",ylab="Cancellations",main= "Cancellations by Airlines of flights from Austin", col= "red", las=3)
plot(TotalFlights ,xlab="Airline ",ylab="TotalFlights",main= "Flights by Airlines of flights from Austin", col= "red", las=3)
plot(TotalFlights ,xlab="Airline ",ylab="TotalFlights",main= "Flights by Airlines of flights from Austin", col= "red", las=3)
FlightsByAirline<-Austin[,c(9,30)]
FlightsByAirline<-AirportInfo[,c(9,30)]
Flight <- na.omit(FlightsByAirline)
Flight$Austin <- 1
TotalFlights<- aggregate(Flight$Austin, by=list(Flight$UniqueCarrier), sum)
plot(TotalFlights ,xlab="Airline ",ylab="TotalFlights",main= "Flights by Airlines of flights from Austin", col= "red", las=3)
par(mfrow=c(1,1))
CancelCarrier$Total = TotalFlights$x
CancelCarrier$Percent <- (CancelCarrier$x/CancelCarrier$Total)*100
plot(CancelCarrier$Group.1, CancelCarrier$Percent,xlab="Airline ",ylab="Cancelleation Percentage",main= "Cancellation percentage of flights from Austin by Airline",  las=3)
library(tm)
library(e1071)
library(rpart)
library(ggplot2)
library(caret)
#reader function
readerPlain = function(fname){
readPlain(elem=list(content=readLines(fname)), id=fname, language='en') }
author_dirs = Sys.glob('./ReutersC50/C50train/*')
file_list = NULL
train_labels = NULL
for(author in author_dirs) {
author_name = substring(author, first=23)
files_to_add = Sys.glob(paste0(author, '/*.txt'))
file_list = append(file_list, files_to_add)
train_labels = append(train_labels, rep(author_name, length(files_to_add)))
}
all_docs = lapply(file_list, readerPlain)
names(all_docs) = file_list
names(all_docs) = sub('.txt', '', names(all_docs))
train_corpus = Corpus(VectorSource(all_docs))
names(train_corpus) = file_list
train_corpus = tm_map(train_corpus, content_transformer(tolower))
train_corpus = tm_map(train_corpus, content_transformer(removeNumbers))
train_corpus = tm_map(train_corpus, content_transformer(removePunctuation))
train_corpus = tm_map(train_corpus, content_transformer(stripWhitespace))
train_corpus = tm_map(train_corpus, content_transformer(removeWords), stopwords("SMART"))
```
- Here I'm putting the training articles into one corpus and cleaning/ tokenizing similarly to the example in class.
- Next comes making a data matrix and removing sparse terms.
```{r}
DTM_train = DocumentTermMatrix(train_corpus)
DTM_train = removeSparseTerms(DTM_train, 0.95)
author_dirs = Sys.glob('./ReutersC50/C50test/*')
file_list = NULL
test_labels = NULL
for(author in author_dirs) {
author_name = substring(author, first=22)
files_to_add = Sys.glob(paste0(author, '/*.txt'))
file_list = append(file_list, files_to_add)
test_labels = append(test_labels, rep(author_name, length(files_to_add)))
}
# Named conversion & cleanup
all_docs = lapply(file_list, readerPlain)
names(all_docs) = file_list
names(all_docs) = sub('.txt', '', names(all_docs))
#Initialize Testing Corpus
test_corpus = Corpus(VectorSource(all_docs))
names(test_corpus) = file_list
#Tokenization of Testing Corpus
test_corpus = tm_map(test_corpus, content_transformer(tolower))
test_corpus = tm_map(test_corpus, content_transformer(removeNumbers))
test_corpus = tm_map(test_corpus, content_transformer(removePunctuation))
test_corpus = tm_map(test_corpus, content_transformer(stripWhitespace))
test_corpus = tm_map(test_corpus, content_transformer(removeWords), stopwords("SMART"))
reuters_dict = NULL
reuters_dict = dimnames(DTM_train)[[2]]
DTM_test = DocumentTermMatrix(test_corpus, list(dictionary=reuters_dict))
DTM_test = removeSparseTerms(DTM_test, 0.95)
DTM_train_df = as.data.frame(inspect(DTM_train))
DTM_test_df = as.data.frame(inspect(DTM_test))
reuters_dict = dimnames(DTM_train)[[2]]
reuters_dict = dimnames(DTM_train)[[2]]
DTM_test = DocumentTermMatrix(test_corpus, list(dictionary=reuters_dict))
DTM_test = removeSparseTerms(DTM_test, 0.95)
DTM_train_df = as.data.frame(inspect(DTM_train))
DTM_test_df = as.data.frame(inspect(DTM_test))
inspect(DTM_test)
model_NB = naiveBayes(x=DTM_train_df, y=as.factor(train_labels), laplace=2)
pred_NB = predict(model_NB, DTM_test_df)
table_NB = as.data.frame(table(pred_NB,test_labels))
conf_NB = confusionMatrix(table(pred_NB,test_labels))
conf_NB
set.seed(1)
DTM_test = as.matrix(DTM_test)
DTM_train = as.matrix(DTM_train)
xx <- data.frame(DTM_test[,intersect(colnames(DTM_test), colnames(DTM_train))])
yy <- read.table(textConnection(""), col.names = colnames(DTM_train), colClasses = "integer")
library(plyr)
DTM_test_clean = rbind.fill(xx, yy)
DTM_test_df = as.data.frame(DTM_test_clean)
library(randomForest)
model_RF = randomForest(x=DTM_train_df, y=as.factor(train_labels), mtry=4, ntree=350)
pred_RF = predict(model_RF, data=DTM_test_clean)
table_RF = as.data.frame(table(pred_RF,test_labels))
conf_RF = confusionMatrix(table(pred_RF,test_labels))
conf_RF$overall
conf_RF_df = as.data.frame(conf_RF$byClass)
conf_RF_df[order(-conf_RF_df$Sensitivity),c(1,2,8)]
georgiaVotes = read.csv('../data/georgia2000.csv', header=TRUE)
author_dirs = Sys.glob('./ReutersC50/C50train/*')
author_dirs
inspect(subset(carts.sorted, subset= confidence > .5 & lift>10))
Carts <- apriori(Transactions, parameter=list(support=.001, confidence=.4, maxlen=5))
carts.sorted <- sort(Carts, by="lift")
inspect(carts.sorted)
detach(package:tm,unload=TRUE)
inspect(Carts)
inspect(subset(carts.sorted, subset= confidence > .5 & lift>10))
inspect(subset(carts.sorted, subset=  lift>10))
detach(package:tm,unload=TRUE)
library(arules)
Transactions<- read.transactions("../data/groceries.txt",format = c("basket"),sep=",")
Transactions
setwd("~/Downloads/STA380Work")
library(knitr)
knit2html('./myNotes.Rmd')
library(knitr)
knit2html('./STA380Assignment2.Rmd')
